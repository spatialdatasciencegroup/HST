{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed514dbc-e76a-46a2-9b09-017d5e142ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "from bisect import bisect_left\n",
    "from scipy.stats import norm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "# import torch.optim as optim\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2158e5ed-0934-4349-b36d-e48bb9b3e7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_WINDOW = 300\n",
    "longest_window = 600\n",
    "\n",
    "regression  = True\n",
    "number_selection = 1 \n",
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda:{}'.format(3))\n",
    "\n",
    "def data_augumentation(train_data, train_label,train_mask,train_down_mat,aug_times = 10):\n",
    "    new_train_data = []\n",
    "    new_train_label = []\n",
    "    \n",
    "    for i in range(len(train_label)):\n",
    "        if(train_label[i]>=5):\n",
    "            new_train_data.extend([train_data[i] for j in range(aug_times)])\n",
    "            new_train_label.extend([train_label[i] for j in range(aug_times)])\n",
    "    \n",
    "    new_train_data = np.array(new_train_data)\n",
    "    new_train_label = np.array(new_train_label)\n",
    "    \n",
    "    train_data = np.concatenate((train_data,new_train_data), axis = 0)\n",
    "    train_label = np.concatenate((train_label, new_train_label), axis = 0)\n",
    "\n",
    "    return train_data, train_label\n",
    "def take_closest(myList, myNumber):\n",
    "    \"\"\"\n",
    "    Assumes myList is sorted. Returns closest value to myNumber.\n",
    "\n",
    "    If two numbers are equally close, return the smallest number.\n",
    "    \"\"\"\n",
    "    pos = bisect_left(myList, myNumber)\n",
    "    if pos == 0:\n",
    "        return 0\n",
    "    if pos == len(myList):\n",
    "        return len(myList) - 1\n",
    "    before = myList[pos - 1]\n",
    "    after = myList[pos]\n",
    "    if after - myNumber < myNumber - before:\n",
    "        return pos\n",
    "    else:\n",
    "        return pos - 1\n",
    "def calCord(latarray, longarray):\n",
    "    xaxis = []\n",
    "    yaxis = []\n",
    "    for i in range(latarray.shape[0]): \n",
    "        lat = latarray[i]\n",
    "        long = longarray[i]\n",
    "        \n",
    "        latMid = (lat+minlat )/2.0\n",
    "\n",
    "\n",
    "        m_per_deg_lat = (111132.954 - 559.822 * math.cos( 2.0 * latMid ) + 1.175 * math.cos( 4.0 * latMid)) / 1e5\n",
    "        m_per_deg_lon = ((3.14159265359/180 ) * 6367449 * math.cos ( latMid ))/1e5\n",
    "\n",
    "        deltaLat = math.fabs(lat - minlat)\n",
    "        deltaLon = math.fabs(long - minlong)\n",
    "        \n",
    "        xaxis.append(deltaLat * m_per_deg_lat)\n",
    "        yaxis.append(deltaLon * m_per_deg_lon )\n",
    "    \n",
    "    return np.array(xaxis), np.array(yaxis) \n",
    "\n",
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def DepthNorm(name, par, depth):\n",
    "    keys = np.load(data_path + name + '_keys.npy')\n",
    "    means = np.load(data_path + name+ '_means.npy')\n",
    "    stds = np.load(data_path + name + '_stds.npy')\n",
    "    \n",
    "    feature_norm = []\n",
    "    for i in range(len(par)):\n",
    "        depth_val = -depth[i]\n",
    "        if not np.isnan(depth_val):\n",
    "            indices = take_closest(keys, depth_val)\n",
    "            #print(indices)\n",
    "        else:\n",
    "            indices = take_closest(keys, -1)\n",
    "        #print(len(indices))\n",
    "        #print(indices)\n",
    "        #assert len(indices) >= 1\n",
    "                                                    \n",
    "        mean_val = means[indices]\n",
    "        std_val = stds[indices]\n",
    "        new_val = (par[i] - mean_val) / (std_val + 1e-3)\n",
    "        feature_norm.append(new_val)\n",
    "        \n",
    "    \n",
    "    return feature_norm\n",
    "\n",
    "def findLargestIndex(time, specific_index):\n",
    "    index = specific_index - 1\n",
    "    while True:\n",
    "        if index < 0 or time[index] < time[specific_index]:\n",
    "            break\n",
    "        else:\n",
    "            index = index - 1\n",
    "    return index\n",
    "\n",
    "def NormalizeDataMultiDim(data):\n",
    "    for i in range(data.shape[1]):\n",
    "        if not np.isnan ((np.max(data[:, i]) - np.min(data[:,i]))):\n",
    "            data[:,i] = (data[:,i] - np.min(data[:,i])) / (np.max(data[:, i]) - np.min(data[:,i]))\n",
    "    return data\n",
    "\n",
    "def shuffle_data_label(data, label):\n",
    "    shuffler = np.random.permutation(len(label))\n",
    "    data_shuffled = data[shuffler]\n",
    "    label_shuffled = label[shuffler]\n",
    "    return data_shuffled, label_shuffled\n",
    "\n",
    "def prepareDataset(features_st,label, TIME_WINDOW): \n",
    "    time = features_st[:, -1]\n",
    "    features_concat = []\n",
    "    label_concat = []\n",
    "    for i in range(features_st.shape[0]):\n",
    "        idx_end = findLargestIndex(time, i)\n",
    "        idx_start = idx_end + 1  - TIME_WINDOW\n",
    "        if idx_start > 0:\n",
    "            features_neigh = features_st[idx_start:idx_end + 1, :]\n",
    "            features_i = features_st[i,:]\n",
    "            features_i = np.expand_dims(features_i, axis = 0)\n",
    "            #features_i = np.expand_dims(features_i, axis = 0)\n",
    "            #features_neigh = np.expand_dims(features_neigh, axis = 0)\n",
    "            features_concat_i = np.concatenate((features_i, features_neigh), axis = 0)\n",
    "            features_concat.append(features_concat_i)\n",
    "            label_concat.append(label[i])\n",
    "    return np.array(features_concat),np.array(label_concat)\n",
    "\n",
    "home_path = '/home/whe2/STGNN/experiment/'\n",
    "with open(home_path + 'data/paired_dataset.pkl', 'rb') as f:\n",
    "    paired_data = pickle.load(f)\n",
    "file_errors_location = home_path + 'data/PinellasMonroeCoKareniabrevis 2010-2020.06.12.xlsx'\n",
    "df = pd.read_excel(file_errors_location)\n",
    "for name in ['par', 'chlor_a', 'Rrs_443','Rrs_469','Rrs_488', 'Kd_490', 'nflh']:\n",
    "    df[name] = 0\n",
    "df = df.sort_values('Sample Date')\n",
    "df_unix_sec = (pd.to_datetime(df['Sample Date'])).astype(int)/10**9\n",
    "df_unix_sec = df_unix_sec.to_numpy()\n",
    "#df_unix = (df_unix_sec - df_unix_sec[0])/(df_unix_sec[3] - df_unix_sec[0])\n",
    "#df['Sample Date'] = df_unix\n",
    "#da_new = df[['Sample Date', 'Sample Depth (m)', 'Latitude','Longitude','Karenia brevis abundance (cells/L)' ]]\n",
    "#da_new = da_new.to_numpy()\n",
    "\n",
    "time_df = df_unix_sec\n",
    "lat_df = df['Latitude']\n",
    "long_df = df['Longitude']\n",
    "concen_df = df['Karenia brevis abundance (cells/L)']\n",
    "\n",
    "length = len(time_df)\n",
    "features_df = df['par'].to_numpy().reshape(length,1)\n",
    "for name in [ 'chlor_a', 'Rrs_443','Rrs_469','Rrs_488', 'Kd_490', 'nflh','Karenia brevis abundance (cells/L)']:\n",
    "    features_df = np.concatenate((features_df, df[name].to_numpy().reshape(length,1)),axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "paired_data = paired_data.sort_values('Sample Date')\n",
    "depth = paired_data['Sample Depth']\n",
    "time_paired = paired_data['Sample Date']\n",
    "time_paired = (pd.to_datetime(time_paired)).astype(int)/10**9\n",
    "time_paired = time_paired.to_numpy() \n",
    "\n",
    "\n",
    "\n",
    "lat_paired = paired_data['Latitude']\n",
    "long_paired = paired_data['Longitude']\n",
    "lat_paired = lat_paired.to_numpy()\n",
    "long_paired = long_paired.to_numpy()\n",
    "maxlat_paired = np.max(lat_paired)\n",
    "minlat_paired = np.min(lat_paired)\n",
    "maxlong_paired = np.max(long_paired)\n",
    "minlong_paired = np.min(long_paired)\n",
    "\n",
    "\n",
    "par = paired_data['par']\n",
    "par = par.to_numpy()\n",
    "chlor_a = paired_data['chlor_a']\n",
    "chlor_a = chlor_a.to_numpy()\n",
    "Rrs_443 = paired_data['Rrs_443']\n",
    "Rrs_443 = Rrs_443.to_numpy()\n",
    "Rrs_469 = paired_data['Rrs_469']\n",
    "Rrs_469 = Rrs_469.to_numpy()\n",
    "Rrs_488 = paired_data['Rrs_488']\n",
    "Rrs_488 = Rrs_488.to_numpy()\n",
    "Kd_490 = paired_data['Kd_490']\n",
    "Kd_490 = Kd_490.to_numpy()\n",
    "nflh = paired_data['nflh']\n",
    "nflh = nflh.to_numpy()\n",
    "\n",
    "data_path = home_path + 'data/depth_stats/'\n",
    "par_norm = DepthNorm('par',par,depth)\n",
    "chlor_a_norm = DepthNorm('chlor_a', chlor_a, depth)\n",
    "Rrs_443_norm = DepthNorm('Rrs_443', Rrs_443, depth)\n",
    "Rrs_469_norm = DepthNorm('Rrs_469', Rrs_469, depth)\n",
    "Rrs_488_norm = DepthNorm('Rrs_488', Rrs_488, depth)\n",
    "Kd_490_norm = DepthNorm('Kd_490', Kd_490, depth)\n",
    "nflh_norm = DepthNorm('nflh', nflh, depth)\n",
    "\n",
    "concen_paired = paired_data['Red Tide Concentration']\n",
    "\n",
    "features_paired = np.stack((par_norm, chlor_a_norm, Rrs_443_norm,Rrs_469_norm,Rrs_488_norm, Kd_490_norm, nflh_norm,concen_paired), axis = 1) \n",
    "#features = NormalizeDataMultiDim(features)\n",
    "#features = np.stack((par, chlor_a, Rrs_443,Rrs_469,Rrs_488, Kd_490, nflh), axis = 1) \n",
    "\n",
    "time = np.concatenate((time_df, time_paired), axis = 0)\n",
    "lat = np.concatenate((lat_df, lat_paired), axis = 0)\n",
    "long = np.concatenate((long_df, long_paired), axis = 0)\n",
    "features = np.concatenate((features_df, features_paired), axis = 0)\n",
    "data_array = np.stack((time, lat, long),axis = 1) \n",
    "data_array = np.concatenate((data_array,features), axis = 1)\n",
    "ind = np.argsort( data_array[:,0] )\n",
    "data_array = data_array[ind]\n",
    "\n",
    "time = data_array[:, 0]\n",
    "time = (time  -  time[0]) / (time[3] - time[0])\n",
    "\n",
    "lat = data_array[:,1]\n",
    "long = data_array[:, 2]\n",
    "features = data_array[:,3:]\n",
    "\n",
    "concen = np.copy(features[:, -1])\n",
    "features[:, -1] = np.log(concen + 1)\n",
    "#concen_mean= np.mean(concen)\n",
    "#concen_std = np.std(concen)\n",
    "#concen = (concen - concen_mean) / concen_std\n",
    "\n",
    "label = np.copy(concen)\n",
    "\n",
    "\n",
    "if regression:\n",
    "    label = np.log10(label+1)\n",
    "    #label = label/np.max(label)\n",
    "else:\n",
    "    label = (label > 1e5)\n",
    "    label = label.astype('uint8')\n",
    "\n",
    "\n",
    "\n",
    "minlat = np.min(lat)\n",
    "minlong = np.min(long)\n",
    "xaxis, yaxis = calCord(np.copy(lat), np.copy(long))\n",
    "locations = np.stack((xaxis, yaxis), axis = 1)\n",
    "locations_latlong =  np.stack((lat, long), axis = 1)\n",
    "\n",
    "\n",
    "def RemoveNan(features, time, locations, locations_latlong, label,concen):\n",
    "    features_new = []\n",
    "    time_new = []\n",
    "    locations_new = []\n",
    "    locations_latlong_new = []\n",
    "    label_new = []\n",
    "    concen_new = []\n",
    "    \n",
    "    for i in range(features.shape[0]):\n",
    "        #flag = 1\n",
    "        for j in range(features.shape[1]):\n",
    "            if np.isnan(features[i,j]):\n",
    "                break;\n",
    "            if (j == features.shape[1] - 1):\n",
    "                features_new.append(features[i,:])\n",
    "                time_new.append(time[i])\n",
    "                locations_new.append(locations[i,:])\n",
    "                locations_latlong_new.append(locations_latlong[i,:])\n",
    "                label_new.append(label[i])\n",
    "                concen_new.append(concen[i])\n",
    "    return np.array(features_new), np.array(time_new), np.array(locations_new),np.array(locations_latlong_new), np.array(label_new), np.array(concen)\n",
    "\n",
    "features, time, locations, locations_latlong, label,concen = RemoveNan(features, time, locations,locations_latlong, label,concen)  \n",
    "\n",
    "time = time.reshape(len(time), 1)\n",
    "features_st = np.concatenate((features, locations_latlong, time), axis = 1) # -1 time, -2 and -3 locations, -4 concen\n",
    "\n",
    "features_data, label_data = prepareDataset(features_st, label, TIME_WINDOW) \n",
    "### final data shape: N*301*11, first 7 feature are spectral feature; then cell concentration; location1, location2, time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6e75b5-829f-4012-8738-3f98064c896c",
   "metadata": {},
   "source": [
    "Data split and augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c2cd0-ebe3-4fc8-a193-89cbf14b8871",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "\n",
    "train_instance = int(np.shape(features_data)[0]*0.7)\n",
    "val_instance = int(np.shape(features_data)[0]*0.8)\n",
    "\n",
    "train_features_data = features_data[:train_instance]\n",
    "train_label = label_data[:train_instance]\n",
    "\n",
    "val_features_data = features_data[train_instance:val_instance]\n",
    "val_label = label_data[train_instance:val_instance]\n",
    "\n",
    "test_features_data = features_data[val_instance:]\n",
    "test_label = label_data[val_instance:]\n",
    "\n",
    "train_features_data, train_label = data_augumentation(train_features_data, train_label, aug_times = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redtide",
   "language": "python",
   "name": "redtide"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
